#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# dsdf_to_olca.py
#
#
# REQUIRED MODULES
#
import concurrent.futures
import json
import logging
import warnings
import io
import os
import re
import time
import uuid
import datetime
from zipfile import ZipFile

import xlrd
import openpyxl
import pandas as pd
import numpy as np
import olca_schema as o
import olca_ipc as olca
import pytz
import requests
from selenium import webdriver   # TO REMOVE/REPLACE w/ EDX
from selenium.webdriver.common.by import By


##############################################################################
# MODULE DOCUMENTATION
##############################################################################
__doc__ = """
A module for semi-automatic translation of NETL DS files to openLCA.

Author:
    James Clarke and Tyler W. Davis
Last Edited:
    2025-08-22

Examples
--------
>>> my_urls = scrape_up_library()
>>> my_url = my_urls[0]
>>> my_url
'https://netl.doe.gov/projects/VueConnection/download.aspx?id=b293514c-6833-4909-90fb-cef9c7551d7b&filename=DS_O_Cement_with_Optional_Capture_2023.03.xlsx'
>>> my_file = get_file_from_url(my_url)
>>> my_file
 'DS_O_Cement_with_Optional_Capture_2023.03.xlsx'
>>> my_data = get_section(my_file, 'SECTION II: PARAMETERS')
>>> my_data.head()
   Parameter Name       Value    Unit         ... References
1	NG_aux_boiler    0.000000   kg/kg cement  ...        2.0
2	elec_in          0.464035   MJ/kg cement  ...        2.0
3	Cansolv_in       0.000000   kg/kg cement  ...        3.0
4	CO2_air          0.844170   kg/kg cement  ...        1.0
5	CO2_prod         0.000000   kg/kg cement  ...        1.0
>>> process_import(my_file) # create openLCA process
"""
__all__ = [
    "SECTION_LABELS",
    "find_section_rows",
    "get_file_from_url",
    "get_section",
    "scrape_up_library",
    "get_metadata",
    "get_refs",
    ### Added 03/13-03/20
    "add_references",
    "get_flows_by_name_and_category",
    "get_flows_by_uuid",
    "get_flows_dict_matcher",
    "get_unit",
    "get_all_units",
    "get_units_by_flow_prop",
    "get_flow_property",
    "create_flow",
    "add_exchange_to_process",
    "add_flow_data",
    "get_flow_data",
    "import_parameters",
    "add_process_dqa",
    "process_import",
    "metadata_import",
    "process_data_scrape",
    ### Added 03/25
    "find_netl_actor",
    "find_us_location",
    "add_process_dqa",
    ### Added 04/02
    "format_parameters",
    "get_up_urls",
    ### Added 04/07
    "search_for_process",
    ### Added 04/08
    "import_pipeline",
    "process_import_multiprocessing",
    # Added 04/10
    "unit_qa_check",
    "get_reference_flow",
    "find_file_in_current_or_parent_children",
    # Added 4/17
    "get_filename_from_process_name"

    # NEED TO ADD UNIT AND FLOW CHECK FUNCTIONS

    # AND FLOW MAPPING FUNCTIONS
    
]


##############################################################################
# GLOBALS
##############################################################################
IPC_PORT = 8080
'''int : The default port number for openLCA IPC service connection.'''
SECTION_LABELS = [
    "SECTION I: META DATA",
    "SECTION II: PARAMETERS",
    "SECTION III: INPUT FLOWS",
    "SECTION IV: OUTPUT FLOWS",
]
'''list : Default section labels for DS files.'''


##############################################################################
# FUNCTIONS
##############################################################################
def _archive_json(data_list, file_path):
    """Write a list of dictionaries to a JSON file.

    Parameters
    ----------
    data_list : list
        A list of dictionaries.
    file_path : str
        A valid filepath to be written to (CAUTION: overwrites existing data)

    Notes
    -----
    Source: taken directly from olca_jsonld_writer.py in ElectricityLCI.
    """
    logging.debug("Writing %d items to %s" % (len(data_list), file_path))
    out_str = ",".join([json.dumps(x.to_dict()) for x in data_list])
    out_str = "[%s]" % out_str
    with open(file_path, 'w') as f:
        f.write(out_str)


def _current_time():
    """Return a ISO-formatted time stamp for right now.

    Returns
    -------
    str
        Time stamp in ISO format.

    Examples
    --------
    >>> _current_time()
    '2023-10-25T21:06:19.200675+00:00'
    """
    return datetime.datetime.now(pytz.utc).isoformat()


def _uid(*args):
    """Generate UUID from the MD5 hash of a namespace identifier and a name.

    This method uses OID namespace, which assumes that the name is an ISO OID.
    Essentially, two strings are hashed together to create a UUID and, if the
    same namespace and path are given again, the same UUID would be returned.

    Warning
    -------
    The UUIDs generated by this method are version 3, which is different
    from standard processes defined elsewhere (e.g., DQSystems and Units).

    Parameters
    ----------
    args : tuple
        A tuple of key words representing a path (order matters).
        The path is a string with each argument separated by a forward slash.
        For flows, the path is 'modeltype.flow', flow name, compartment, and
        unit.
        For processes, the path is 'modeltype.process', process category,
        location, and name.

    Returns
    -------
    str
        A version 3 universally unique identifier (UUID)

    Notes
    -----
    Taken from USEPA's ElectricityLCI (olca_jsonld_writer.py)
    """
    path = '/'.join([str(arg).strip() for arg in args]).lower()
    return str(uuid.uuid3(uuid.NAMESPACE_OID, path))

def get_filename_from_process_name(invert = False):
    """Gets filename for all DS file processes in openLCA (provided that the process names = the metadata name), or vice versa.

    Parameters
    --------
    invert : bool (Default: False)
        T = invert k/v such that process_names[process name] = filename
        F = do not invert k/v such that process_names[filename] = process name

    Notes
    -----
    I believe there was one duplicate process name which had to be manually handled.
    
    >>> Value 'Ocean Freighter Transport' is used by keys: ['DS_Stage3_O_Ocean_Freighter_Transport_WindTurbine_2010.01.xlsx', 'DS_Stage24_O_Ocean_Freighter_Transport_2010.02.xlsx'] # If not handled manually.
    
    """
    
    # Directory to scan
    directory = "/Users/jimmyclarke0812/Desktop/coding/ds file conversion/Entire UP library from Tyler - 040325"
    
    # Dict to store process names for files that we have downloaded
    process_names = {}
    
    # Loop over all files in directory
    for filename in os.listdir(directory):
        if filename.endswith(".xls") or filename.endswith(".xlsx"):
            filepath = os.path.join(directory, filename)
            try:
                metadata = get_metadata(filepath)
                # Look for the row where Name == 'Process Name'
                process_name_row = metadata[metadata['Name'] == 'Process Name']
                if not process_name_row.empty:
                    process_name = process_name_row['Value'].values[0]
                    process_names[filename] = process_name
                else:
                    print(f"'Process Name' not found in {filename}")
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    # Optional: print or save results
    print(f"Extracted {len(process_names.keys())} process names.")


    if not invert:
        return process_names

    if invert:
        process_names = {v: k for k, v in process_names.items()}
        return process_names


def unit_qa_check():
    """Identifies and prompts user to replace empty or corrupted units in the database.

    Notes
    -----
    - FYI: if you have the database open, do not refresh the process or flow or other objects or you might break the database!
    - if you break the process during addition of exchanges to a single process you already started modifying, it might break due to flow changes(bug)
    - removed "next" and "end" for now
    
    """
    client = olca.Client()
    response = ''
    
    # All processes
    # all_processes = [client.get(o.Process,name= "Operation of NETL Baseline SubCPC Power Plant")] # just one for testing
    all_processes = client.get_all(o.Process)
    
    # Grab all units and flow props
    all_units, all_unit_synonyms = get_all_units(client)
    all_flow_props = client.get_all(o.FlowProperty)
    
    # For all processes
    for p in all_processes:
        p_new = p

        try:
            qrf = get_reference_flow(p)

        except:
            pass
        # For each exchange...
        for e in p_new.exchanges:
            if e.unit is None:
                # If there's no unit...
                print(f"{e.flow.name} in {p_new.name} has no unit.\nFlow description: {e.description}\nFlow formula: {e.amount_formula}") # default unit (e.g., unit shows up as blank in the exchange list)
                try:
                    print(f"Functional unit: {qrf.amount} {qrf.unit.name} of {qrf.flow.name}")
                except:
                    print(f"No functional unit.")
                
                try:
                    # Get parameter description (which includes unit info usually) to help fill in the unit
                    param_name = e.amount_formula.split("*")[1]
                    
                    for param in p_new.parameters:
                        if param.name == param_name:
                            print(f"Parameter description: {param.description}")
                except:
                    print("Could not find parameter description.")
    
                # Get flow obj for the exchange
                flow_obj = client.get(o.Flow, e.flow.id)
                    
                while True:
                    ## Get unit list and show user
                    flow_prop_factors = flow_obj.flow_properties
                    flow_props = [client.get(o.FlowProperty,fpf.flow_property.id) for fpf in flow_prop_factors]
                    units_dict = get_units_by_flow_prop(flow_props, client)[0]
                    units_list = list(units_dict.values())
                    units_list = [item.lower() for sublist in units_list for item in sublist] # flatten/lower units_list of lists (nested)
                    print(f"List of units:\n{units_list}\n")
    
                    # Get reference flow prop, reference unit
                    ref_fp_ref_unit = None
                    
                    for fp in flow_prop_factors:
                        if fp.is_ref_flow_property:
                            ref_fp_ref_unit = fp.flow_property.ref_unit
                        
        
                    # prompt user to fill in missing unit
                    valid_choices = units_list
                
                    response = input(f"Enter a unit to fill the gap, or type \"afp\" to add a flow property:").strip().lower()
                    
                    if response in valid_choices:
                        break
    
                    # elif response == "next":
                    #     break
    
                    # elif response == "end":
                    #     break
    
                    elif response == "afp":
                        # Print list of flow props
                        all_flow_prop_names = [fp.name for fp in all_flow_props]
                        print(f"List of flow properties:\n{all_flow_prop_names}")
                        
                        # Keep running, do not break
                        fp_to_add_name = input(f"Enter the name of the flow property you would like to add to the flow:")
                        
                        # Add flow property and unit to flow object
                        try:
                            # Get fp ref
                            fp_ref = client.get(o.FlowProperty,name=fp_to_add_name).to_ref()
    
                            # Get ref flow unit for new fp
                            new_flow_prop_factor = client.get(o.FlowProperty,name=fp_to_add_name)
    
                            # Get ref unit for new fp
                            new_ref_fp_ref_unit = client.get(o.UnitGroup,new_flow_prop_factor.unit_group.id).default_flow_property.ref_unit
                            
                            
                            # Get factor
                            print("Flow property factors work as follows:\nIf X is the reference flow prop, and Y is the new flow prop, Z in X = Z*Y is the factor.")
                            print(f"Using reference units for the relevant flow properties for {e.flow.name}:  {ref_fp_ref_unit} = Z*{new_ref_fp_ref_unit}")
                            fp_to_add_factor = input(f"Enter the flow property factor (Z, above) to convert to the reference flow property:")
    
                            # Create fpf to add to the flow (not the exchange)
                            fpf_to_add = o.FlowPropertyFactor(
                                conversion_factor=fp_to_add_factor,
                                flow_property = fp_ref,
                                is_ref_flow_property = False
                            )
                            
                            
                            # Add flow property to flow
                            flow_obj.flow_properties.append(fpf_to_add)
    
                            # Add flow property to exchange
                            e.flow_property = fp_ref
                            # print(e.flow_property)
                            
                            # Commit to database
                            # client.delete(flow_obj)
                            # time.sleep(3)
                            client.put(flow_obj)
                            print(f"{fp_to_add_name} successfully added to {e.flow.name} flow object.")
                            print("-" * 20) # Prints 20 hyphens
    
                        except Exception as _e:
                            print("Could not add flow property to flow.")
                            print(_e)
                            
                    else:
                        print(f"'{response}' is not a valid unit. Please try again.")
    
                # # Skipping
                # if response == "next":
                #     continue
    
                # elif response == "end":
                #     break
                
                # Add flow unit to exchange
                e.unit = get_unit(response, all_units, all_unit_synonyms, client).to_ref()
                # print(e.unit)
    
                if type(e.unit) is o.Ref:
                    print(f"Successfully selected: {response}")   
                    print("-" * 20) # Prints 20 hyphens
                else:
                    print(f"Exchange unit object not type o.Ref!\n{e.unit}")
                    break
    
        # if response == "end":
        #     break
    
        # print(p_new.exchanges)
        
        if client.get(o.Process, p_new.id) != p:
            # If changed
            
            try:
                client.delete(p) # This is what breaks the database badly. Why?
                time.sleep(1)
                client.put(p_new) # Doesn't update if we don't delete.
                print("Successfully updated client process.")
                print("=" * 20+"\n") # Prints 20 hyphens
            except Exception as _e:
                print("Client not updated.")
                print(_e)
    
    # if response == "end":
    #     print("Ended by user.")
    # else:
    #     print("Units fixed.")


def get_reference_flow(p:o.Process):
    """Returns quantitative reference Exchange() object for a process.
    """
    
    e_list = p.exchanges
    ref_flow = None

    for e in e_list:
        if e.is_quantitative_reference:
            ref_flow = e
            return ref_flow

    raise Exception(f"No reference flow found for process {p.name}")


def process_import_multiprocessing(start=None, end=None, case_list=None, max_workers=8):
    """Executes the `import_pipeline` function concurrently over a range or list of indices using multithreading.

    Parameters
    ----------
    start : int, optional
        The starting index (1-based) for processing.
    end : int, optional
        The ending index (exclusive) for processing.
    case_list : list of int, optional
        An explicit list of 1-based indices to process.
    max_workers : int, optional
        The maximum number of concurrent threads to use. Default is 8.

    Returns
    -------
    None
        Results are printed to the console. Errors are caught and logged for each task.
    """
    client = olca.Client()
    up_urls = get_up_urls()
    warnings.simplefilter("ignore")

    if case_list is not None:
        task_indices = [i - 1 for i in case_list]  # convert to 0-based
    elif start is not None and end is not None:
        task_indices = list(range(start - 1, end))  # also 0-based
    else:
        raise ValueError("Either provide start and end, or case_list.")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {
            executor.submit(import_pipeline, i): i for i in task_indices
        }

        for future in concurrent.futures.as_completed(future_to_index):
            ds_index = future_to_index[future]
            try:
                result = future.result()
                print(result)
            except Exception as e:
                print(f"[{ds_index + 1}] UNCAUGHT ERROR: {e}")


def import_pipeline(ds_count):
    """Executes the import process for a specific dataset based on its index.

    This function retrieves a dataset file from a predefined URL list, processes the
    import, and returns a formatted status message indicating success or failure,
    along with the elapsed time for the operation.

    Parameters
    ----------
    ds_count : int
        The index of the dataset to import. This index is used to retrieve the
        appropriate URL from the `up_urls` list (every dataset is assumed to be
        located at every other entry, i.e., ds_count * 2).

    Returns
    -------
    str
        A message indicating the result of the import. The message includes the dataset
        name (if retrievable), the outcome (SUCCESS, FAILED, or RETURNED NONE), and
        the time taken for the import in seconds.

    Notes
    -----
    - Assumes `up_urls`, `get_file_from_url()`, and `process_import()` are defined elsewhere.
    - Errors are caught and returned in the status message instead of raising exceptions.
    - Logging is done via `print()` for dataset name and via return values for summary output.

    Examples
    --------
    >>> result = import_pipeline(0)
    >>> print(result)
    [1] SUCCESS: dataset_name (3.21s)
    """

    client = olca.Client()
    up_urls = get_up_urls()
    warnings.simplefilter("ignore")
    
    start_time = time.time()
    try:
        ds_url = up_urls[ds_count * 2]
        ds_name = ds_url.split("filename=")[1].rsplit(".xlsx")[0]
        print(f"[{ds_count + 1}] {ds_name}")

        ds_file = get_file_from_url(ds_url)
        p = process_import(ds_file)

        elapsed = round(time.time() - start_time, 2)
        if p is not None:
            return f"[{ds_count + 1}] SUCCESS: {p.name} ({elapsed}s)"
        else:
            return f"[{ds_count + 1}] IMPORT RETURNED NONE ({elapsed}s)"

    except Exception as e:
        elapsed = round(time.time() - start_time, 2)
        return f"[{ds_count + 1}] IMPORT FAILED: {e} ({elapsed}s)"


def find_file_in_current_or_parent_children(target_filename: str) -> str | None:

    # 0. If it's an existing full or partial path, return it directly
    if os.path.isfile(target_filename):
        return os.path.abspath(target_filename)

    # 1. Get just the base filename to search for
    base_filename = os.path.basename(target_filename)

    current_dir = os.getcwd()
    parent_dir = os.path.dirname(current_dir)

    # 2. Check current directory
    for f in os.listdir(current_dir):
        if f == base_filename:
            return os.path.join(current_dir, f)

    # 3. Check each child of parent directory
    for child in os.listdir(parent_dir):
        child_path = os.path.join(parent_dir, child)
        if os.path.isdir(child_path):
            for f in os.listdir(child_path):
                if f == base_filename:
                    return os.path.join(child_path, f)

    # Not found
    return None


def search_for_process(filename):
    """Searches for a process in the database and prints a log of the results.
    
    Parameters
    ---------
    filename : str
        .xlsx filename, which contains a process name

    Returns
    -------
    True
        If process exists, False otherwise
        
    Prints
    ------
    If found
        "Process <process_name: str> already exists in database.
        ...Skipping...
    If not found
        "Process <process_name: str> not found in database.
        ...Importing...
        
    """
    client = olca.Client()

    path = find_file_in_current_or_parent_children(filename)

    md_dict_init = get_metadata(path)
    
    process_to_import_name = md_dict_init[md_dict_init['Name']=="Process Name"]['Value'].values[0]
        
    try:
        p_obj = client.get(o.Process,name=process_to_import_name)
    except Exception as e:
        pass

        
    if type(p_obj) == o.Process:
        print(f"Process \"{process_to_import_name}\" already exists in database with uuid {p_obj.id}.")
        return True
    else:
        # Process doesn't exist
        print(f"Process \"{process_to_import_name}\" not found in database. \n")
        return False


def format_parameters(ds_inputs, ds_outputs, ds_parameters):
    ''' Edits parameters into a format that is acceptable by openLCA.

    Added 04/02/25 - James Clarke

    Parameters
    ---------
    ds_parameters : pd.DataFrame
        Relevant columns: 'Parameter Name', 'Formula', 'Value'
    ds_inputs : pd.DataFrame
        Relevant columns: 'Parameter'
    ds_outputs : pd.DataFrame
        Relevant columns: 'Parameter'
    
    Returns
    -------
    ds_inputs_edited : pd.DataFrame
        ds_inputs, edited, acceptable by openLCA
    ds_outputs_edited : pd.DataFrame
        ds_outputs, edited, acceptable by openLCA
    ds_parameters_edited : pd.DataFrame
        ds_parameters, edited, acceptable by openLCA

    Notes
    ------
    1) Removes parameters with text values
    2) Handles parameter names starting with numbers
    3) Updates inputs, outputs
    4) Passes updated DFs on for further use
    '''
    # Bypass
    if ds_parameters.empty:
        # If no params, no need to format
        return ds_inputs, ds_outputs, ds_parameters
    
    # CLONE
    ds_parameters_edited = ds_parameters # clone
    ds_inputs_edited = ds_inputs 
    ds_outputs_edited = ds_outputs

    # Edits - CONVERTING TO LOWERCASE
    ds_parameters_edited['Parameter Name'] = ds_parameters_edited['Parameter Name'].str.lower()
    try:
        ds_parameters_edited['Formula'] = ds_parameters_edited['Formula'].str.lower()
    except:
        pass

    # Outputs are blank error handling
    try:
        ds_outputs_edited['Parameter'] = ds_outputs_edited['Parameter'].str.lower()
    except  Exception as e:
        # Possible that outputs is empty? As in DS_Stage1_C_Fracturing_Fluid_Manufacture_2015.01
        # print(f"Error {e} has occurred while converting output flow 'Parameters' column to lowercase (are output flows blank?) ")
        pass

    # Inputs are blank error handling
    try:
        ds_inputs_edited['Parameter'] = ds_inputs_edited['Parameter'].str.lower()
    except Exception as e:
        # Possible that inputs is empty? As in DS_NG_Storage_Fugitives_2018
        # print(f"Error {e} has occurred while converting input flow 'Parameters' column to lowercase (are input flows blank?)")
        pass

    
    # Addressing error in old DFs (missing 'Value' column in parameters section...)

    # Convert 'US' to 'Value' if exists (seems to happen for Uranium processes)
    # Should do nothing most of the time.
    if "Value" not in ds_parameters.columns:
        ds_parameters.rename(columns={'US': 'Value'}, inplace=True)
    
    if "Value" not in ds_parameters.columns:
        # DS_CTG_Uranium_EU_Enrich_2011-02.xls has "Europe"...
        ds_parameters.rename(columns={'Europe': 'Value'}, inplace=True)
    
    # If at this point, there's still no "Value" column, name the first column between formula and units to 'Value'
    # Find index positions of 'Formula' and 'Units'
    if "Value" not in ds_parameters.columns:
        col_list = ds_parameters.columns.tolist()
    
        if "Formula" in col_list and "Units" in col_list:
            start = col_list.index("Formula") + 1
            end = col_list.index("Units")
    
            ds_parameters.rename(columns={col_list[start]: "Value"}, inplace=True)
    
    # HELPER FUNCTIONS
    # Step 1: Identify all parameter names that start with a number or include a period or space
    def does_param_include_period_or_space(param):
        return bool(re.match(r'^\d', param) or re.search(r'[ .]', param))

    def is_param_starting_with_number(param):
        return bool(re.match(r'^\d', param))
    
    ### [A] Remove parameters with text values ###
    # Note: skipped text parameters will not be updated if they exist in ds_inputs or ds_outputs. They shouldn't be in there anyway.
    # (i) REMOVE PARAMS W/ TEXT VALUES

    # Convert 'Value' to numeric, coercing errors to NaN
    ds_parameters_edited['Value'] = pd.to_numeric(ds_parameters_edited['Value'], 
                                                  errors='coerce')

    # Drop non-numeric values and capture them in case of error handling downstream
    try:
        skipped_parameters = ds_parameters_edited[ds_parameters_edited['Value'].isna()]['Parameter Name'].values[0] # capture rows to drop
    except:
        pass
    
    ds_parameters_edited = ds_parameters_edited.dropna(subset=['Value']) # Drop non-numeric rows in 'Value'
    ds_parameters_edited = ds_parameters_edited.reset_index(drop=True) # Reset index

    # Warn user of parameters to be skipped
    # warnings.warn(f"""Parameter(s) {skipped_parameters}  contain non-numeric values and will be skipped. Process validation recommended""", UserWarning)

    
    ### [B] Handle parameter names starting with numbers ###
        
    # Store a mapping of original and modified names
    name_mapping = {}
    
    for param in ds_parameters_edited['Parameter Name']:
        if is_param_starting_with_number(param):
            new_name = 'var_' + param  # Modify the parameter name to start with 'var_'
            name_mapping[param] = new_name

    # Step 2: Update all relevant DataFrames
    # Update Parameter Names in ds_parameters
    ds_parameters_edited['Parameter Name'] = ds_parameters_edited['Parameter Name'].replace(name_mapping) # WORKS
    
    # Update formulas in ds_parameters
    for index, row in ds_parameters_edited.iterrows():
        for old_name, new_name in name_mapping.items():
            if isinstance(row['Formula'], str):
                if old_name in row['Formula'] and ("var_"+old_name) not in row['Formula']:  # Check if parameter name is used in formula
                    ds_parameters_edited.at[index, 'Formula'] = row['Formula'].replace(old_name, new_name)
                    
                    # Update row!
                    row = ds_parameters_edited.iloc[index]
    
    # Step 3
    # Update references in ds_inputs and ds_outputs
    for df in [ds_inputs_edited, ds_outputs_edited]:
        for index, row in df.iterrows():
            try:
                df.at[index,'Parameter'] = row['Parameter'].replace(name_mapping)
            except:
                # Name_mapping might be empty
                pass
    
    ### [3] Replace spaces and periods with underscores ###
    # Added 04-03-25 due to numerous typos in DS files
    
    # Step 1: Identify all parameter names that start with a number, space, or period
    def is_param_starting_with_number_space_or_period(param):
        return bool(re.match(r'^[\d .]', param))
    
    # Store a mapping of original and modified names
    name_mapping_2 = {}
    
    for param in ds_parameters_edited['Parameter Name']:
        if does_param_include_period_or_space(param):
            new_name = param.replace(' ', '_').replace('.', '_')  # Modify the parameter name to remove "." or " "
            name_mapping_2[param] = new_name

    # Update Parameter Names
    ds_parameters_edited['Parameter Name'] = ds_parameters_edited['Parameter Name'].replace(name_mapping_2) # WORKS

    # Update formulas
    for index, row in ds_parameters_edited.iterrows():
        for old_name, new_name in name_mapping_2.items():
            try:
                if old_name in row['Formula']:  # Check if parameter name is used in formula
                    ds_parameters_edited.at[index, 'Formula'] = row['Formula'].replace(old_name, new_name)
            except:
                # Will fail on independent params without formulas!
                pass

    # Update references in ds_inputs and ds_outputs
    for df in [ds_inputs_edited, ds_outputs_edited]:
        df['Parameter'] = df['Parameter'].replace(name_mapping_2)

    ### [4] Replace [] => () in formulas
    # Added 04-03-25 due to numerous typos in DS files
    # Update formulas
    for index, row in ds_parameters_edited.iterrows():
        try:
            ds_parameters_edited.at[index, 'Formula'] = row['Formula'].replace("[", "(").replace("]",")")
        except:
            # Will fail on independent params without formulas!
            pass
    # No need to update refs in inputs and outputs
        
    return ds_inputs_edited, ds_outputs_edited, ds_parameters_edited


# BUG: Function parameters should not start with underscores;
# by definition, they are important.
def add_exchange_to_process(_uuid,
                            p,
                            _flow_data,
                            _all_units,
                            units_by_flow_property,
                            units_by_flow_property_synonyms,
                            _ids,
                            _all_units_synonyms,
                            client):
    """Append flow to process exchange list.

    Parameters
    ----------
    _uuid : str
        UUID identifier for the desired flow in FEDEFL
    p : o.Process
        openLCA process object
    _flow_data : dict
    _all_units : dict
        Container of all units passed down for speed.
        _all_units = get_all_units(client)
    units_by_flow_property :
        Another container of all units passed down for speed.
        units_by_flow_property = get_units_by_flow_prop(client)
    units_by_flow_property_synonyms : dict
        The second return object from :func:`get_units_by_flow_prop`.
    _ids : dict
        Dictionary containing refs to all flows in database
        _ids = get_flows_dict(client)
    _all_units_synonyms : ?
    client : olca.Client
        IPC server reference to current openLCA client.

    Examples
    --------
    >>> # TODO
    >>> client = o.Client()
    >>> all_units = get_all_units(client)
    >>> units_by_flow_property = get_units_by_flow_prop(client)
    >>> id_list = get_flows_dict(client)
    >>> id = _ids[0]
    >>> add_exchange_to_process(_id, )

    Notes
    ------
    Works for all elementary flows and existing technosphere
    flows. Error handling done upstream with "try/except" method
    on UUID search.
    """
    # Basic build
    # NOTE: Internal IDs in exchanges are consecutively numbered starting with
    # one. This in an openLCA internal ID that allows for the same flow to
    # exist as multiple exchanges (e.g., between different providers).
    # They must be consecutively ordered and unique.
    _flow_to_add = o.Exchange(
        amount = _flow_data['_amount'],
        internal_id = 1, # see comment above
        is_avoided_product = False,
        is_input = _flow_data['_is_input'],
        is_quantitative_reference= _flow_data['_is_reference'],
        amount_formula = _flow_data['_formula']
    )

    # Description
    # Giving issues with nans for blanks
    _flow_to_add.description = _flow_data['_comment']
    
    if (type(_flow_to_add.description) is np.nan) or (type(_flow_to_add.description) is float):
        # Is blank
        _flow_to_add.description = ''

    # Error handling
    try:
        # Obtain flow reference
        _flow_to_add.flow = _ids[_uuid]
        _flow_to_add.flow.ref_unit = _flow_data['_units']

        # Obtain flow object
        _flow_to_add_object = client.get(o.Flow, _uuid)

        # Add flow property reference (unfortunately not documented in DS file
        # ...using default for flow)
        for _prop_factor in _flow_to_add_object.flow_properties:
            if _prop_factor.is_ref_flow_property:
                _flow_to_add.flow_property = _prop_factor.flow_property.to_ref()

        # Add unit reference
        unit_object = get_unit(
            _flow_data['_units'], _all_units, _all_units_synonyms, client
        )
        _flow_to_add.unit = unit_object.to_ref()

        # Add DQI
        try:
            if _flow_data['_dqi'] != 'nan':
                _flow_to_add.dq_entry = "("+_flow_data['_dqi'].replace(",",";")+")"
        except:
            pass

    except Exception as e:
        # raise ValueError(f"Error adding flow {_uuid} to the database.
        # Check flow, flow property, and unit existence.")
        print(f"{e}. {_flow_to_add.flow.name} not added to {p.name}.")
        return None

    p.exchanges.append(_flow_to_add)


def add_flow_data(_flow_data_all, p, client, fresh_start=False):
    """Convert DS file flow list from :func:`get_flow_data` to a list of
    exchanges in p, in the client database.

    Parameters
    ---------
    _flow_data_all : list
        Flow data (List of Dict objects)
    p : o.Process
    client: o.Client
    fresh_start : bool
        T/F parameter to determine if p.exchanges will be cleared before
        loading more flows. Defaults to false.

    Returns
    -------
    p : o.Process

    Notes
    ------
    client.put(p) will commit process "p" to the client database with the new
    set of flows.

    **THIS FUNCTION IS VERY SLOW - NEEDS TO BE SPEED OPTIMIZED**
    Accomplished using _ids and other data passes to avoid as many IPC calls
    as possible.
    """
    # Run these calls once and pass down
    _all_units, _all_units_synonyms = get_all_units(client)
    flow_properties = client.get_all(o.FlowProperty)
    units_by_flow_property, units_by_flow_property_synonyms, flow_prop_obj = get_units_by_flow_prop(flow_properties, client)
    _refs, _ids, _name_compartment = get_flows_dict_matcher(client)

    # Log
    print("...importing flow data...")

    # Instantiate "exchanges" attribute in process "p" to blank list
    if fresh_start:
        p.exchanges = []

    for _flow_data in _flow_data_all:
        # Query for uuid (returns None if not found)
        try:
            _uuid = get_flows_by_name_and_category(_flow_data['_flow_name'], _flow_data['_flow_compartment'], _name_compartment).id
        except Exception as e:
            print(e)

        ### MATCH UUID OR CREATE FLOW DECISION TREE###
        if _flow_data['_flow_type'] == "Elementary flows":
            # Elementary flow
            # print(_flow_data['_flow_name'])
            # print(_flow_data['_flow_compartment'])

            # Add flow
            add_exchange_to_process(
                _uuid,
                p,
                _flow_data,
                _all_units,
                units_by_flow_property,
                units_by_flow_property_synonyms,
                _ids,
                _all_units_synonyms,
                client
            )

        else:
            # Technosphere Flow (capital F!)
            # print(_flow_data['_flow_name'])
            # print(_flow_data['_flow_compartment'])

            # if _uuid is not None:
            try:
                # Flow exists -> link it
                _uuid = get_flows_by_name_and_category(
                    _flow_data['_flow_name'],
                    _flow_data['_flow_compartment'],
                    _name_compartment
                ).id
                add_exchange_to_process(
                    _uuid,
                    p,
                    _flow_data,
                    _all_units,
                    units_by_flow_property,
                    units_by_flow_property_synonyms,
                    _ids,
                    _all_units_synonyms,
                    client
                )
            except:
                # Flow doesn't exist -> Create flow, link it
                _uuid = create_flow(
                    _flow_data,
                    units_by_flow_property,
                    units_by_flow_property_synonyms,
                    _all_units,
                    _all_units_synonyms,
                    client
                ) # Create flow in database

                # Add flow to _ids dictionary
                _ids[_uuid] = client.get(o.Flow,_uuid)
                # Add flow to _name_compartment dictionary
                _name_compartment[_flow_data['_flow_compartment']+"/"+_flow_data['_flow_name']] = client.get(o.Flow,_uuid)

                add_exchange_to_process(
                    _uuid,
                    p,
                    _flow_data,
                    _all_units,
                    units_by_flow_property,
                    units_by_flow_property_synonyms,
                    _ids,
                    _all_units_synonyms,
                    client
                )

    return p


def add_process_dqa(_md_dict, _process, _client):
    """Handle process DQAMs that were manually imported into openLCA from EPA.

    Parameters
    ---------
    _md_dict : Dict
        Contains metadata from DS file
    _process : o.Process
    _client : olca.Client

    Notes
    -----
    Uses data from md_dict pulled from DS file to populate openLCA DQI entry
    field.
    """
    # Get all data quality assurance matrices in openLCA database
    _dq_systems = _client.get_all(o.DQSystem)
    source_list = []

    # HOTFIX: add support for missing DQSystems; pulls from FedCommons
    if len(_dq_systems) == 0:
        _dq_systems, source_list = get_dqsystem()

        logging.info("Adding EPA DQI matrices to database")
        for _dqs in _dq_systems:
            _client.put(_dqs)
        for source in source_list:
            _client.put(source)

    # Find index for 'process pedigree matrix' without hardcoding
    _dq_system_sorter = [
        _dqs.name == "US EPA - Process Pedigree Matrix" for _dqs in _dq_systems]

    _process_dqam = _dq_systems[_dq_system_sorter.index(True)]
    _flow_dqam = _dq_systems[_dq_system_sorter.index(False)]

    # Add DQAMs to process
    _process.dq_system = _process_dqam.to_ref()
    _process.exchange_dq_system = _flow_dqam.to_ref()

    # Add process DQAM values
    try:
        # Doesn't work if md_dict[DQI] is already None (no DQI process data)
        _process_dqi_values = _md_dict['DQI'].replace(',',';') #str processing
        _process.dq_entry = '('+_process_dqi_values+')' #commit to process
    except:
        _process.dq_entry = None


def add_references(_ds_file, _process, client):
    """Add references in _ref_df to _process in client.

    Parameters
    ----------
    _ds_file : str
        Filename
    _process : o.Process
    client : olca.Client

    Returns
    -------
    o.Process

    Notes
    -----
    1.  Search for source.
        Shouldn't take long to O(n) search because there aren't many.
    2.  Find source, or create source.
    3.  Add source to process.

    Follow naming convention for sources from openLCA Best Practices Document.
    """
    # Initialize
    if _process.process_documentation is None:
        # If there isn't any other documentation yet, initialize the attribute
        _process.process_documentation = o.ProcessDocumentation()

    # Log
    print("...adding references...")

    # Initialize sources list
    _process.process_documentation.sources = []

    # Get reference data dataframe
    try:
        _ref_data = get_refs(_ds_file)
    except Exception as e:
        print(f"Failure in add_references:\n{e}")

    if _ref_data.empty:
        # If the Reference Source Info tab does not exist, exit function (older DS files don't have the tab)
        print("...'Reference Source Info' tab not available. No sources imported...")
        return

    # Get sources
    _sources = client.get_all(o.Source)
    _sources_dict = {_source.name: _source.id for _source in _sources}

    # Fields
    _field_names = list(_ref_data.loc[:,0].values)

    for i in _ref_data.iloc[:,1:]:
        # Fields
        # All DS files have different degrees of reference documentation, it seems...

        # ERROR handling...
        try:
            _year = str(_ref_data[i][_field_names.index('Year')])
        except:
            _year = 'nan'
        
        try:
            _title = str(_ref_data[i][_field_names.index('Title')])
        except:
            _title = 'nan'
            
        try:
            _bib_text = str(_ref_data[i][_field_names.index('BibliographicText')])
        except:
            _bib_text = 'nan'

        try:
            _author = str(_ref_data[i][_field_names.index('FirstAuthor')])
        except:
            _author = 'nan'

        # Ideal format based on openLCA best practices documentation
        _source_name = _author + ". (" + _year + "). " + _title
        _full_citation = _bib_text
        
        # Check for nans
        if (_year == 'nan') and (_author == 'nan') and (_title != 'nan') and (_bib_text == 'nan'):
            # No data, we've reached the end, stop importing
            break

        # ...in source name
        if (_year == 'nan') or (_author == 'nan'):
            if _title != 'nan':
                _source_name = _title
            else:
                if _bib_text != 'nan':
                    _source_name = _bib_text
                else:
                    # don't add this one, it's missing too much data
                    continue

        # ... in description
        if _bib_text == 'nan':
            _full_citation = ''

        # Search for source
        if _source_name not in _sources_dict.keys():
            # If not exist => create source
            _new_source_obj = o.Source(name=_source_name,
                                       description=_full_citation)
            # Try/except block
            try:
                client.put(_new_source_obj)
                
                _process.process_documentation.sources.append(
                    _new_source_obj.to_ref()
                )
            except Exception as e:
                # Generally a json nan error
                print(f"{_new_source_obj.name} failed to put to database\n{e}")

        else:
            # Does exist => locate and add existing
            _source_uuid = _sources_dict[_source_name]
            _source_obj = client.get(o.Source,_source_uuid)
            _process.process_documentation.sources.append(_source_obj.to_ref())

    return _process


# BUG: don't use underscores for parameters
# NOTE: While a best practice, I prefer not to use annotations in functions,
# unless you are willing and able to do it for all functions.
def create_flow(_flow_data,
                units_by_flow_property,
                units_by_flow_property_synonyms,
                _all_units,
                _all_units_synonyms,
                client):
    """Create a flow in openLCA database specified by the `client`.

    Parameters
    ----------
    _flow_data : dict
        Contains flow data for the new flow object
    units_by_flow_property : dict
        For example, ``units_by_flow_property = get_units_by_flow_prop(client)``
    units_by_flow_property_synonyms : dict
        For example,
        ``_, units_by_flow_property_synonyms = get_units_by_flow_prop(client)``
    _all_units : dict
        All units in database
    _all_units_synonyms : dict
    client : olca.Client
        IPC server reference to current openLCA client

    Returns
    -------
    str
        UUID for new flow, as a string

    Notes
    -----
    Used when DS file flow does not already exist in the database.
    """
    # Create a new flow
    _new_flow = o.Flow()
    _new_flow.category = _flow_data['_flow_compartment']
    _new_flow.name = _flow_data['_flow_name']
    _new_flow.id = str(uuid.uuid4())

    # Flow type
    if _flow_data['_flow_name'] == 'Elementary flows':
         _new_flow.flow_type = o.FlowType.ELEMENTARY_FLOW

    else:
        _new_flow.flow_type = o.FlowType.PRODUCT_FLOW # Technosphere flow

    # Unit reference
    _new_flow.unit = get_unit(
        _flow_data['_units'], _all_units, _all_units_synonyms, client).to_ref()

    # Populate flow property
    _new_flow.flow_properties = []
    _flow_property_to_add = get_flow_property(
        _new_flow.unit.name,
        units_by_flow_property,
        units_by_flow_property_synonyms,
        client
    )
    _flow_property_to_add.ref_unit = _new_flow.unit.name

    _flow_property_factor_to_add = o.FlowPropertyFactor(
        conversion_factor = 1.0,
        flow_property = _flow_property_to_add,
        is_ref_flow_property=True
    )
    _new_flow.flow_properties.append(_flow_property_factor_to_add)

    client.put(_new_flow) # Add to database

    # Return UUID
    return _new_flow.id


def find_in_metadata(md, p_name):
    # IN PROGRESS
    #
    # Return the value for a metadata field.
    try:
        p_val = None
        tmp = md.loc[md['Name'] == p_name, 'Value'].reset_index(drop=True)
        if len(tmp) == 1:
            p_val = tmp.loc[0]
    
        return p_val
    except Exception as e:
        print(f"Error in find_in_metadata function\n{e}")


def find_netl_actor(client):
    '''Find UUID for first "NETL LCA Team" o.Actor object, or create one if
    none exist.

    Parameters
    ---------
    client : o.Client

    Returns
    -------
    str
        UUID of the actor.

    '''

    _all_actors = client.get_all(o.Actor) # List

    # Find NETL LCA Team actor

    for _actor in _all_actors:
        if _actor.name == 'NETL LCA Team':
            netl_actor_uuid = _actor.id
            return netl_actor_uuid

    # If none exists, create one

    netl_actor_object = o.Actor(
        id = _uid(),
        name = 'NETL LCA Team'
    )

    client.put(netl_actor_object)

    return netl_actor_object.id


def find_section_rows(filepath,
                      sheet_name="Data Summary",
                      section_labels=None,
                      end_marker="End of List"):
    """Find row numbers associated with specific sections in an Excel sheet.

    Parameters
    ----------
    filepath : str
        The path to the Excel file.
    sheet_name : str, optional
        The name of the sheet to process. Defaults to "Data Summary".
    section_labels : list, optional
        A list of section labels to search for in column B.
        If None, defaults to ["Section I", "Section II", "Section III",
        "Section IV", "Section V"].
    end_marker : str, optional
        The text that marks the end of a section in column C.
        Defaults to "End of Line".

    Returns
    -------
    dict
        A dictionary where keys are section labels and values are lists of
        row numbers where that section label is found. Returns an empty
        dictionary if any errors occur. Prints error messages to console.

    Notes
    -----
    Created by Gemini AI.

    Updated 04/08/25 by James Clarke to handle .xls files using xlrd.

    Examples
    --------
    >>> filepath = "your_excel_file.xlsx"  # Replace with your file path
    >>> section_data = find_section_rows(filepath)

    >>> if section_data:
    ...     for section, rows in section_data.items():
    ...         print(f"{section}: Rows {rows}")
    ... else:
    ...     print("No section data found. Check for errors.")
    >>> # Example with custom section names:
    >>> custom_sections = [
    ...     "Start of Section A", "Middle of Section A", "End of Section A"]
    >>> section_data = find_section_rows(
    ...     filepath, section_labels=custom_sections)
    >>> if section_data:
    ...     for section, rows in section_data.items():
    ...         print(f"{section}: Rows {rows}")
    ... else:
    ...     print("No section data found. Check for errors.")
    """
    if section_labels is None:
        section_labels = SECTION_LABELS

    if filepath.rsplit(".")[-1] == "xlsx":
        # .xlsx file

        try:
            # Open in read-only mode for efficiency
            workbook = openpyxl.load_workbook(filepath, read_only=True)
    
            if sheet_name not in workbook:
                print(f"Error: Sheet '{sheet_name}' not found in the workbook.")
                return {}
    
            sheet = workbook[sheet_name]
    
            # Initialize dictionary
            section_rows = {label: [] for label in section_labels}
    
            current_section = None

            # Set a max row
            if sheet.max_row < 5000:
                end_row = sheet.max_row
            else:
                end_row = 5000
            
            for row_index in range(1, end_row + 1):  # Rows are 1-indexed
                cell_value = sheet.cell(row=row_index, column=2).value # Column B
                eol_value = sheet.cell(row=row_index, column=3).value  # Column C
    
                if cell_value in section_labels:
                    current_section = cell_value
                    section_rows[cell_value].append(row_index)
    
                if current_section and eol_value == end_marker:
                    section_rows[current_section].append(row_index)
                    current_section = None  # reset for next section
    
            return section_rows
        except FileNotFoundError:
            print(f"Error: File '{filepath}' not found.")
            return {}
        except Exception as e:
            print(f"An error occurred: {e}")
            return {}

    elif filepath.rsplit(".")[-1] == "xls":
        # Largely the same as the above but indexing is different so copied the entire rewritten code in the interest of time.
        try:
            workbook = xlrd.open_workbook(filepath)
    
            if sheet_name not in workbook.sheet_names():
                print(f"Error: Sheet '{sheet_name}' not found in the workbook.")
                return {}
    
            sheet = workbook.sheet_by_name(sheet_name)
    
            # Initialize dictionary
            section_rows = {label: [] for label in section_labels}
    
            current_section = None
            for row_index in range(sheet.nrows):
                cell_value = sheet.cell_value(row_index, 1)  # Column B (0-indexed)
                eol_value = sheet.cell_value(row_index, 2)   # Column C
    
                if cell_value in section_labels:
                    current_section = cell_value
                    section_rows[cell_value].append(row_index + 1)  # Convert to 1-indexed
    
                if current_section and eol_value == end_marker:
                    section_rows[current_section].append(row_index + 1)
                    current_section = None
    
            return section_rows

        except FileNotFoundError:
            print(f"Error: File '{filepath}' not found.")
            return {}
        except Exception as e:
            print(f"An error occurred: {e}")
            return {}
    else:
        raise ValueError("Invalid file type. Only '.xls' or '.xlsx' files are supported.")
            


def find_us_location(client):
    '''Find UUID for first "United States - US" location, or create one if
    none exist.

    Parameters
    ---------
    client : olca.Client

    Returns
    -------
    str
        The UUID of the US location object.
    '''

    _all_locs = client.get_all(o.Location) # List

    # Find US location
    for _loc in _all_locs:
        if _loc.name == 'United States':
            us_loc_uuid = _loc.id
            return us_loc_uuid # Break

    # If none exists, create one
    us_loc_object = o.Location(
        id = _uid(),  # BUG, probably should use a standard UUID
        name = 'United States',
        code = 'US'
    )

    client.put(us_loc_object)

    return us_loc_object.id


def get_all_units(client):
    """Get dictionary of units from the client.

    Parameters
    ---------
    client : olca.Client

    Returns
    -------
    tuple
        A tuple of length two.

        -   dict, the keys are unit names (e.g., 'kj') and values are unit
            objects.
        -   dict, the keys are unit synonyms as 'a; b; c;' strings and
            values are unit objects.

    Examples
    --------
    >>> client_units, client_unit_synonyms = get_all_units(client)
    """
    _all_unit_groups = client.get_all(o.UnitGroup)
    # HOTFIX missing unit groups and flow properties [TWD; 20250325]
    if len(_all_unit_groups) == 0:
        _all_unit_groups, _all_flow_properties = get_units_and_properties()
        for ug in _all_unit_groups:
            client.put(ug)
        for fp in _all_flow_properties:
            client.put(fp)

    client_units = {}
    client_unit_synonyms = {}

    for _unit_group in _all_unit_groups:
        for _unit_object in _unit_group.units:
            # lowercase to be case insensitive
            client_units[_unit_object.name] = _unit_object
            if isinstance(_unit_object.synonyms, list):
                # If list, continue
                client_unit_synonyms[
                    ";".join(_unit_object.synonyms)
                ] = _unit_object

    return client_units, client_unit_synonyms


def get_dqsystem():
    """Return list of GreenDelta's DQSystem and Source objects.

    This utilizes the Federal LCA Commons' public API to pull data from the
    core database repository.

    A local copy of the LCA Commons' DQSystems and Source objects
    is either accessed (in user's data directory) or created (using requests).

    Notes
    -----
    This method writes up to two files locally:

    -   dq_systems.json
    -   dq_sources.json

    Returns
    -------
    tuple
        A tuple of length two.
        First item is a list of 2 olca-schema DQSystem objects.
        Second item is a list of 1 olca-schema Source objects.
    """
    # Define the base URL for the public API
    url = (
        "https://www.lcacommons.gov/"
        "lca-collaboration/ws/public/download/json"
    )

    # Token for JSON-LD with data quality indicators
    token_url = url + (
        "/prepare/Federal_LCA_Commons/Fed_Commons_core_database?path=DQ_SYSTEM"
    )

    # Save data to the local folder.
    data_dir = "."

    d_file = "dq_systems.json"
    d_path = os.path.join(data_dir, d_file)
    d_list = []

    s_file = "dq_sources.json"
    s_path = os.path.join(data_dir, s_file)
    s_list = []

    if not os.path.exists(d_path) or not os.path.exists(s_path):
        # Pull from Federal Elementary Flow List
        logging.info("Reading data from Federal LCA Commons")
        token = requests.get(token_url).content.decode()
        r = requests.get(f"{url}/{token}")
        if r.ok:
            with ZipFile(io.BytesIO(r.content)) as z:
                for name in z.namelist():
                    # Note there are only two folders in the zip file:
                    # 'dq_systems' and 'sources'
                    if name.startswith("dq_systems") and name.endswith("json"):
                        d_dict = json.loads(z.read(name))
                        d_obj = o.DQSystem.from_dict(d_dict)
                        d_list.append(d_obj)
                    elif name.startswith("sources") and name.endswith("json"):
                        s_dict = json.loads(z.read(name))
                        s_obj = o.Source.from_dict(s_dict)
                        s_list.append(s_obj)
        else:
            logging.error(
                "Failed to access Elementary Flow List on Fed Commons!")

        # Archive to avoid running requests again.
        _archive_json(d_list, d_path)
        logging.info("Saved DQSystems from LCA Commons to JSON")

        _archive_json(s_list, s_path)
        logging.info("Saved DQI sources from LCA Commons to JSON")

    # Only read locally if needed (i.e., if data wasn't just downloaded)
    if os.path.exists(d_path) and len(d_list) == 0:
        logging.info("Reading DQSystems from local JSON")
        with open(d_path, 'r') as f:
            my_list = json.load(f)
        for my_item in my_list:
            d_list.append(o.DQSystem.from_dict(my_item))

    if os.path.exists(s_path) and len(s_list) == 0:
        logging.info("Reading DQI sources from local JSON")
        with open(s_path, 'r') as f:
            my_list = json.load(f)
        for my_item in my_list:
            s_list.append(o.Source.from_dict(my_item))

    return (d_list, s_list)


def get_file_from_url(url):
    """Download file (e.g., DS/DF file) from NETL VUE website if not found locally.

    This method searches the parent directory and all subdirectories within it
    before downloading the file.

    Parameters
    ----------
    url : str
        A web site URL pointing to a file on the NETL VUE website.

    Returns
    -------
    str
        The file path if found or downloaded.
    """
    # Extract filename from the URL query parameters
    match = re.search(r'filename=([^&]+)', url)
    filename = match.group(1) if match else "downloaded_file.xlsx"
    
    # Get the parent directory of the current working directory
    parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))
    
    # Search for the file in the parent directory and its subdirectories
    for root, _, files in os.walk(parent_dir):
        if filename in files:
            file_path = os.path.join(root, filename)
            logging.info(f"File exists! Located at: {file_path}")
            return file_path
    
    # If file is not found, proceed with downloading
    logging.info("Downloading file from the web")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Referrer": "https://netl.doe.gov/"
    }
    response = requests.get(url, headers=headers, stream=True)
    
    if response.status_code != 200:
        print(f"Failed to download file. Status code: {response.status_code}")
        return None
    
    # Save the file in the current directory
    with open(filename, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    msg = f"File downloaded successfully as {filename}"
    logging.info(msg)
    return os.path.join(os.getcwd(), filename)

def get_flow_data(flow_df, toggle, client):
    """Convert input/output flows from DS file dataframe into a data structure.

    Parameters
    ----------
    flow_df : pd.DataFrame
        Flow dataframe
    toggle : bool
        Toggles input vs output flow dataframe (unspecified in dataframe itself)
        T = input
        F = output
    client : o.Client
        Unused.

    Returns
    -------
    _flow_data_all : List
        List of dictionaries of flow data, each element representing a single flow

    Notes
    -----
    1. Iterate through rows of DS file dataframe
    2. Convert dataframe series into list of dicts

    """
    # Data for all flows
    _flow_data_all = []

    for _index, _flow in flow_df.iterrows():

        # Data for a single flow
        _flow_data = {}

        # Get flow name and compartment (Capitalization is different for elementary flows)
        _flow_data['_name'] = _flow['Flow Name'].rstrip()
        _flow_data['_name'] =  _flow_data['_name'].replace('Elementary Flows','Elementary flows')
        
        # Split name and compartment at the last "/"
        if re.search(r"/", _flow_data['_name']):
            _flow_data['_split_name'] = _flow_data['_name'].rsplit("/", 1)
        else:
            # No /
            _flow_data['_split_name'] = _flow_data['_name']
        
        # Remove trailing spaces
        if type(_flow_data['_split_name']) is list:
            _flow_data['_flow_name'] = _flow_data['_split_name'][1].rstrip()
        else:
            # Just one string, no "/" to split
            _flow_data['_flow_name'] = _flow_data['_name']
        
        # Split name at the last "/" to get flow type
        if re.search(r"/", _flow_data['_name']):
            _flow_data['_flow_type'] = _flow_data['_name'].split("/", 1)[0].rstrip()
            _flow_data['_flow_compartment'] = _flow_data['_split_name'][0].rstrip()
        else:
            # No /
            # Technosphere
            _flow_data['_flow_type'] = 'Technosphere Flows'
            _flow_data['_flow_compartment'] = 'Technosphere Flows'

        # Get units
        # So finicky with units/unit groups/ flow properties, etc. in openLCA
        if (type(_flow['Units per RF']) is not float) and (type(_flow['Units']) is not float):
            # If both exist, and there's a space or / in RF, use the shorter one (so kg rather than kg/kg NG etc.). Seems to happen sometimes.

            if ' ' in _flow['Units per RF'] or '/' in _flow['Units per RF']:
                
                if len(_flow['Units per RF']) >= len(_flow['Units']):
                    _flow_data['_units'] = _flow['Units'].rstrip()
                else:
                    _flow_data['_units'] = _flow['Units per RF'].split('/')[0]
            else:
                _flow_data['_units'] = _flow['Units per RF'].rstrip()
        else:
            try:
                # If not, take whichever works...
                _flow_data['_units'] = _flow['Units per RF'].rstrip()
            except:
                # If blank (nan, float) throws error
                # If this is empty too, well then we have no unit...
                _flow_data['_units'] = _flow['Units'].rstrip()

        # Parsing formula/value/total
        _flow_data['_parameter'] = _flow['Parameter']
        _flow_data['_value'] = _flow['Value']

        if _flow_data['_parameter'] is not np.nan:
            # Relies on parameter => has formula decision tree
            # All the "Total" values take the format:
            #      "value" (always 1) * parameter value
            try:
                _flow_data['_formula'] = str(_flow_data['_value']) + "*" + _flow_data['_parameter']
            except Exception as e:
                # print(f"Error occurred in parsing flow formulas for {_flow_data['_flow_name']}. Formula set to None.\n{e}")
                # No parameter
                _flow_data['_formula'] = None
        else:
            # No parameter
            _flow_data['_formula'] = None

        # Flow amount
        _flow_data['_amount'] = _flow['Total']
        
        # Metadata
        try:
            _flow_data['_dqi'] = str(_flow['DQI']).rstrip()
        except:
            _flow_data['_dqi'] = np.nan
        _flow_data['_comment'] = _flow['Comments']

        # is_input and is_reference
        if toggle:
            # Input flow; for all input flows, reference is false
            _flow_data['_is_reference'] = False
            _flow_data['_is_input'] = True
        else:
            # toggle = False, Output flow
            if _index == 0:
                # First output is the reference flow
                _flow_data['_is_reference'] = True
            else:
                # Other outputs are not the ref flow
                _flow_data['_is_reference'] = False

            _flow_data['_is_input'] = False

        # Add to list
        _flow_data_all.append(_flow_data)

    return _flow_data_all


def get_flow_property(unit,
                      units_by_flow_property,
                      units_by_flow_property_synonyms,
                      client):
    """Get a flow property reference for a given unit string.

    Parameters
    ---------
    unit: str
        E.g., 'kg*km'
    units_by_flow_property : dict
        Get dict: d[flow prop uuid] = [unit1, unit2, ... , etc.]
    units_by_flow_property_synonyms : dict
        d[flow prop uuid] = [syn1, syn2, etc.]
    client : olca.Client

    Returns
    -------
    o.Ref
        Reference object for the flow property.
    """
    for flow_prop_uuid in units_by_flow_property.keys():
        # Get list of units from flow property uuid
        units = units_by_flow_property[flow_prop_uuid]
        # Get list of unit synonyms from flow property uuid
        _synonyms = units_by_flow_property_synonyms[flow_prop_uuid]

        # If found: populate reference for return
        if unit.lower() in [u.lower() for u in units]: # case insensitive
            flow_property_obj = client.get(o.FlowProperty, flow_prop_uuid)

            # Reference for new exchange w/ correct flow property
            return flow_property_obj.to_ref()

        # If above fails, search in synonyms for unit
        if unit.lower() in [s.lower() for s in _synonyms]: # case insensitive
            flow_property_obj = client.get(o.FlowProperty, flow_prop_uuid)

            # Reference for new exchange w/ correct flow property
            return flow_property_obj.to_ref()

    # Throw error if unit not found!
    raise ValueError(
        f"Error: valid FlowProperty for {unit} not found in the database")


def get_flows_by_name_and_category(_name, _category, _name_compartment):
    """Get flow reference object by UUID from a given dictionary.

    Parameters
    ----------
    _name : str
        Name to search for.
    _category: str
        Category to search for.
    _name_compartment : dict
        A data structure containing all indexes to Ref() attributes for all
        flows indexed by "category/name" in _refs.

    Returns
    -------
    o.Ref
        Flow reference object.
    """
    try:
        return _name_compartment[_category+"/"+_name]
    except:
        return None
        # raise ValueError(f"Flow {_category+"/"+_name} not found in flow dictionary generated in database.")


def get_flows_by_uuid(_uuid, _ids):
    """Get flow reference object by UUID from a given dictionary.

    Parameters
    ----------
    _uuid : str
        UUID to search for.
    _ids : dict
        Dictionary generated by :func:`get_flows_dict_matcher`.

    Returns
    -------
    o.Ref
        Flow reference object.
    """
    try:
        return _ids[_uuid]
    except:
        raise ValueError(
            f"UUID {_uuid} not found in flow dictionary generated in database.")


def get_flows_dict_matcher(client):
    """Retrieve all flows from openLCA and return dictionaries.

    Parameters
    ----------
    client : olca.Client
        The openLCA IPC client.

    Returns
    ------
    tuple
        A tuple of length three.

        -   list, a list of all flow Refs.
        -   dict, a data structure containing all indexes to Ref() attributes
            for all flows indexed by uuid in _refs.
        -   dict, a data structure containing all indexes to Ref() attributes
            for all flows indexed by "category/name" in _refs.

    Notes
    ------
    Will be used to match excel flows -> uuids in FEDEFL (if matches exist)
    in O(1) time.
    """
    # Get flows
    # NOTE: Much faster than get_all() - Get list of all flows
    _refs = client.get_descriptors(o.Flow)

    # Get attributes for UUIDs and category/name
    _ids = {ref.id: ref for ref in _refs}
    _name_compartment = {ref.category+"/"+ref.name: ref for ref in _refs}

    return _refs, _ids, _name_compartment


def get_metadata(work_book, as_dict=False):
    """Scrape metadata from NETL DS file

    Parameters
    ----------
    work_book : str
        The file path to an NETL DS Excel workbook
    as_dict : bool, optional
        Whether to return the metadata as a dictionary.

    Returns
    -----
    metadata_df: pandas.DataFrame
        A list of all metadata fields in 'Info' tab and 'Data Summary' tab
        including 'Section I: Metadata' with the columns 'Name' and 'Value'
        (all values are strings) with the following name categories (for meta
        data). Note some of the Name categories may be different depending on
        the template.

        - 'Additional Notes',
        - 'Allocation Applied',
        - 'Completeness', o.ProcessDocumentation.completeness_description
        - 'DQI',
        - 'Date Created', o.ProcessDocumentation.creation_date
        - 'Description',
        - 'Disclaimer',
        - 'Files',
        - 'Flows Aggregated in Data Set',
        - 'Functional unit: Amount',
        - 'Functional unit: Material',
        - 'Functional unit: Unit',
        - 'Geographical Coverage', o.Process.location
        - 'How to Cite This Document',
        - 'Point of Contact',
        - 'Process Description',
        - 'Process Name', o.Process.name
        - 'Process Scope',
        - 'Process Type',
        - 'Region',
        - 'Revision History',
        - 'Summary and Calculations Worksheets',
        - 'Template Version',
        - 'Year Data Best Represents',

    Notes
    -----
    Disclaimer in 'Info' tab is in a textbox, and cannot be parsed without emulating python using pywin32 or similar. Other text boxes also exist.
    """
    ### From Tyler's 1/30/25 coding hour ###

    #
    # INFO TAB
    #
    # Scrape 'Info' tab (Hardcoded -- same exact cell locations/names from 2019-2024)
    gen_info_df = pd.read_excel(
        work_book,
        sheet_name="Info",
        skiprows=2,
        usecols="C:D",
        nrows=4,
        header=None
    )
    gen_info_df.columns = ["Name", "Value"]


    ### Expansions from James - 02/13/25 ###

    #
    # INFO TAB, CONTINUED
    #
    # Load the spreadsheet
    df = pd.read_excel(work_book, sheet_name="Info", usecols="B:C")  # Load relevant columns

    # Define section labels to search for
    section_labels_info = [
        "Date Created:",
        "Point of Contact:",
        "Revision History:",
        "How to Cite This Document:",
        "Additional Notes:",
        "Disclaimer:",
        "Template Version:"
    ]

    # Find the row indices where these section labels appear
    section_indices = df[df.iloc[:, 0].isin(section_labels_info)].index.tolist()

    # Extract data between each section
    section_data = {}
    for i in range(len(section_indices)):
        start_idx = section_indices[i]
        end_idx = section_indices[i+1] if i+1 < len(section_indices) else len(df)

        # Store label and corresponding data
        label = df.iloc[start_idx, 0]
        data = df.iloc[start_idx+1:end_idx, 1].dropna().tolist()  # Extract non-null values from column C

        # Join multiline text into a single string
        section_data[label] = " ".join(map(str, data))

    # Convert to DataFrame
    df_sections = pd.DataFrame(list(section_data.items()), columns=["Name", "Value"])

    #
    # DATA SUMMARY TAB
    #
    # These data elements are consistent in cell location from 2009-2024
    # Goal and scope is a textbox in DS format v1...which must be parsed manually (if desired)

    # Reference flow
    ref_flow_df = pd.read_excel(
        work_book,
        sheet_name="Data Summary",
        skiprows=4,
        usecols="C:G",
        nrows=1,
        header=None
    )

    ref_flow_df = ref_flow_df.transpose()
    ref_flow_df = ref_flow_df.rename(columns={
        ref_flow_df.columns.values[0]: 'Value'
    })
    # Keep the functional unit in separate properties for easier extraction.
    ref_flow_df['Name'] = [
        np.nan,
        'Functional unit: Amount',
        'Functional unit: Unit',
        np.nan,
        'Functional unit: Material'
    ]
    ref_flow_df = ref_flow_df[['Name','Value']] # Cleaning

    # Description
    desc_df = pd.read_excel(
        work_book,
        sheet_name="Data Summary",
        skiprows=5,
        usecols="D",
        nrows=1,
        header=None
    )

    desc_df['Name'] = 'Description'
    desc_df = desc_df.rename(columns={desc_df.columns.values[0]: 'Value'})
    desc_df = desc_df[['Name','Value']] # Cleaning

    # DQI
    try:
        DQI_df = pd.read_excel(
            work_book,
            sheet_name="Data Summary",
            skiprows=4,
            usecols="K:N",
            nrows=1,
            header=None
        )
    
        # Had to adjust slightly to work with all DS file versions
        DQI_df = DQI_df.dropna(axis=1, how='any').drop(
            DQI_df.columns[DQI_df.iloc[0].values == 'DQI'][0],
            axis=1
        )
        DQI_df['Name'] = 'DQI'
        DQI_df = DQI_df.rename(columns={DQI_df.columns.values[0]: 'Value'})
        DQI_df = DQI_df[['Name','Value']] # Cleaning
    except:
        # Some models don't have process DQI data
        DQI_df = None

    # SECTION I: META DATA
    metadata_table_df = pd.read_excel(
        work_book,
        sheet_name="Data Summary",
        skiprows=9,
        usecols="B,D",
        nrows=8,
        header=None
    )

    metadata_table_df = metadata_table_df.rename(
        columns = {
            metadata_table_df.columns.values[0]: 'Name',
            metadata_table_df.columns.values[1]: 'Value'
        }
    ) # Cleaning

    #
    # COMBINING ALL METADATA INTO ONE DATAFRAME
    #
    metadata_df = pd.concat([gen_info_df, df_sections, ref_flow_df, desc_df, DQI_df, metadata_table_df],axis=0) # Vertical concat
    metadata_df = metadata_df.dropna(how='all').reset_index(drop=True)

    # Final clean-up step (remove white space and semicolons from name column)
    metadata_df['Name'] = metadata_df['Name'].str.strip().str.rstrip(":")

    if as_dict:
        return metadata_to_dict(metadata_df)
    else:
        return metadata_df


def get_refs(work_book):
    """Get reference data from NETL DS file in tab: 'Reference Source Info'.

    Parameters
    ----------
    work_book : str
        The file path to an NETL DS Excel workbook.

    Returns
    -------
    pandas.DataFrame
        A DataFrame including all reference fields in 'Reference Source Info'
        incl. blank columns. DataFrame column 1 includes the following fields:

        - 'Number'
        - 'SourceType'
        - 'Title'
        - 'FirstAuthor'
        - 'AdditionalAuthors'
        - 'Year'
        - 'Date'
        - 'PlaceOfPublication'
        - 'Publisher'
        -'PageNumbers'
        -'Table or Figure Number'
        -'NameOfEditors'
        -'TitleOfAnthology'
        -'Journal'
        -'VolumeNo'
        -'IssueNo'
        -'Docket Number'
        -'Copyright'
        -'Internet Address'
        -'Data Type (Origin)'
        -'Year Data Represents'
        -'Geographical Representation'
        -'Representativeness'
        -'BibliographicText'
        -'Text/Description'

    """
    try:
        ds_refs = pd.read_excel(
            work_book,
            header = None,
            sheet_name='Reference Source Info',
            skiprows=1,
            nrows=28) # Read references from Excel (changed from 24-28 to acommodate version 3.0 of DS file)
    
        return ds_refs
    except Exception as e:
        # print("Data scrape failed in get_refs. No references will be imported:")
        # print(e)
        
        # Empty DF, no refs.
        ds_refs = pd.DataFrame()
        return ds_refs


def get_section(work_book, section_title, work_sheet='Data Summary'):
    """Return data from NETL DS file for a given section in tab: 'Data Summary'

    Parameters
    ----------
    work_book : str
        The file path to an NETL DS Excel workbook
    section_title : str
        One of the section titles from the Data Summary worksheet,
        see ``SECTION_LABELS`` for valid options.
    work_sheet : str, optional
        The worksheet name, by default 'Data Summary'

    Returns
    -------
    pandas.DataFrame
        A data frame with rows and columns associated with the section data.

    Raises
    ------
    KeyError
        If a section title not known is given by the user.

    Notes
    ------
    Not intended for use in 'Section I: Meta Data'. Use get_metadata() instead.
    """
    # A little bit of error handling for our sake
    if section_title not in SECTION_LABELS:
        raise KeyError("Section, '%s', not defined!" % section_title)

    # Pulling the indices from the worksheet
    # Add if/else based on work_book extension (.xls vs .xlsx)
    my_idx = find_section_rows(work_book)

    # Get the staring and ending indices for user-defined section
    start_idx, end_idx = my_idx[section_title]

    # Compute the rows
    #   1 for the money; two for the skip; three for the header
    n_rows = end_idx - start_idx - 3
    ds = pd.read_excel(
        work_book,
        sheet_name=work_sheet,
        skiprows=start_idx + 1,
        nrows=n_rows,
    )
    # Clean up step.
    # Edited 04/08, to only drop unnamed columns if there aren't named columns following it
    cols = ds.columns.tolist()
    
    # Find index of last 'Unnamed' column
    unnamed_indices = [i for i, col in enumerate(cols) if col.startswith("Unnamed:")]
    
    # Iterate through each unnamed index
    for idx in unnamed_indices:
        # Check if there are any named columns before and after this unnamed column
        named_before = any(not col.startswith("Unnamed:") for col in cols[:idx])
        named_after = any(not col.startswith("Unnamed:") for col in cols[idx + 1:])
        
        # Drop if there are no named columns before or after the unnamed column
        if not named_before or not named_after:
            ds = ds.drop(columns=[cols[idx]])

    # drop_cols = [x for x in ds.columns if x.startswith("Unnamed:")] # Original lines
    # ds = ds.drop(columns=drop_cols)
    ds = ds.dropna(how='all')

    return ds


def get_unit(_unit, _all_units, _all_units_synonyms, client):
    """Get unit uuids in openLCA client (typically hidden) from unit groups.

    Parameters
    ---------
    _unit : str
    _all_units: dict
        Get dictionary list of all units d['unit_name'] = 'unit_id',
        ``_all_units = get_all_units(client)``
    _all_units_synonyms : dict
    client : olca.Client
        Unused.

    Returns
    -------
    o.Ref
        Reference object to "unit object" which doesn't really exist in openLCA.

    Notes
    -----
    Speed improvement gained by passing _all_units rather than calculating
    separately for every function call.

    Two of the input parameters can be obtained from the following:
    ``_all_units, _all_units_synonyms = get_all_units(client)``.
    """
    # Lowercase to compare case insensitive
    _unit_lower = _unit.lower()

    # Convert unit to something in the database
    manual_unit_map = {
        "locomotives":"pcs",
        "locomotive":"pcs",
        "dragline":"pcs",
        "shovel":"pcs",
        "loader":"pcs",
        "conveyor":"pcs",
        "drill":"pcs",
        "crusher":"pcs",
        "silo":"pcs",
        "truck":"pcs",
        "kg-km":"kg*km",
        "kg/MWh":"kg",
        "piece/kg":"piece",
        "kg/kg yellowcake":"kg",
        "pcs/kg yellowcake":"pcs",
        "pcs/MWh":"pcs",
        "piece/kg UO2":"pcs",
        "piece/kg uo2":"pcs",
        "pcs/kg NG":"pcs",
        "pcs/kg ng":"pcs",
        "pcs/kg":"pcs",
        "kg/pcs":"kg",
        "shields":"pcs",
        "line pans":"pcs",
        "head drives":"pcs",
        "tail drives":"pcs",
        "shearers":"pcs",
        "stage loaders":"pcs",
        "kg/mwh":"kg",
        "pcs/mwh":"pcs",
        "kg/m2":"kg",
        "piece/kg uo2":"pcs",
        "pcs/kg ng":"pcs",
        "pcs/ kg":"pcs",
        "pcs/kg fuel":"pcs",
        "stacker":"pcs",
        "railcars":"pcs",
        "bq/mwh":"bq"
                      }

    if _unit_lower in manual_unit_map.keys():
        # Replace unit if in list above (else do nothing)
        _unit_lower = manual_unit_map[_unit_lower]

    for _unit_name ,unit_object in _all_units.items():
        # Comparing case insensitive rather than querying d[key]
        # If doesn't exist in database, manually create it.
        if _unit_name.lower() == _unit_lower:
            return unit_object

    # Else, search by synonyms, which we can manually add to if needed.
    for _unit_synonyms ,unit_object in _all_units_synonyms.items():
         # Comparing case insensitive
        _unit_synonyms = _unit_synonyms.lower()

        if _unit_lower in _unit_synonyms:
            return unit_object

    # Else, throw error:
    raise ValueError(f"Error: valid Unit for {_unit_lower} not found in the database")


def get_units_and_properties():
    """Return list of GreenDelta's unit group and flow property objects.

    This utilizes the Federal LCA Commons' public API to pull data from the
    Elementary Flow List repository.

    A local copy of the LCA Commons' Federal Elementary Flow List unit groups
    is either accessed (in eLCI's data directory) or created (using requests).

    Notes
    -----
    This method writes up to two files locally:

    -   flow_properties.json
    -   unit_groups.json

    This method is taken directly from olca_jsonld_writer.py in ElectricityLCI.

    Returns
    -------
    tuple
        A tuple of length two.
        First item is a list of 27 olca-schema UnitGroup objects.
        Second item is a list of 33 olca-schema FlowProperty objects.
    """
    # Define the base URL for the public API
    url = (
        "https://www.lcacommons.gov/"
        "lca-collaboration/ws/public/download/json"
    )

    # Using "path=FLOW_PROPERTY" obtains all Flow properties and unit groups
    token_url = url + (
        "/prepare/Federal_LCA_Commons/elementary_flow_list?path=FLOW_PROPERTY"
    )

    # Save locally
    data_dir = "."

    u_file = "unit_groups.json"
    u_path = os.path.join(data_dir, u_file)
    u_list = []

    p_file = "flow_properties.json"
    p_path = os.path.join(data_dir, p_file)
    p_list = []

    if not os.path.exists(u_path) or not os.path.exists(p_path):
        # Pull from Federal Elementary Flow List
        logging.info("Reading data from Federal LCA Commons")
        token = requests.get(token_url).content.decode()
        r = requests.get(f"{url}/{token}")
        if r.ok:
            with ZipFile(io.BytesIO(r.content)) as z:
                # Find the unit groups, convert them to UnitGroup class
                for name in z.namelist():
                    # Note there are only three folders in the zip file:
                    # 'flow_properties', 'flows', and 'unit_groups';
                    # we want the 27 JSON files under unit_groups
                    # and the 33 JSON files under flow_properties.
                    if name.startswith("unit") and name.endswith("json"):
                        u_dict = json.loads(z.read(name))
                        u_obj = o.UnitGroup.from_dict(u_dict)
                        u_list.append(u_obj)
                    elif name.startswith("flow_") and name.endswith("json"):
                        p_dict = json.loads(z.read(name))
                        p_obj = o.FlowProperty.from_dict(p_dict)
                        p_list.append(p_obj)
        else:
            logging.error(
                "Failed to access Elementary Flow List on Fed Commons!")

        # Archive to avoid running requests again.
        _archive_json(u_list, u_path)
        logging.info("Saved unit groups from LCA Commons to JSON")

        _archive_json(p_list, p_path)
        logging.info("Saved flow properties from LCA Commons to JSON")

    # Only read locally if needed (i.e., if data wasn't just downloaded)
    if os.path.exists(u_path) and len(u_list) == 0:
        logging.info("Reading unit groups from local JSON")
        with open(u_path, 'r') as f:
            my_list = json.load(f)
        for my_item in my_list:
            u_list.append(o.UnitGroup.from_dict(my_item))

    if os.path.exists(p_path) and len(p_list) == 0:
        logging.info("Reading flow properties from local JSON")
        with open(p_path, 'r') as f:
            my_list = json.load(f)
        for my_item in my_list:
            p_list.append(o.FlowProperty.from_dict(my_item))

    return (u_list, p_list)


def get_units_by_flow_prop(flow_properties, client):
    """Get dictionary of units by flow property for the client.

    Parameters
    ---------
    flow_properties : list
        flow_properties = client.get_all(o.FlowProperty)
    client : olca.Client

    Returns
    -------
    tuple
        A tuple of length three.

        -   dict, the keys are UUIDS for flow properties and values are
            lists of unit names as strings.
        -   dict, the keys are UUIDS for flow properties and values are
            lists of unit synonyms as strings.
        -   dict, the keys are UUIDS for flow properties and values are
            flow property objects for reference.

    Examples
    --------
    >>> (flow_property_units,
    ...  flow_property_units_synonyms,
    ...  flow_property_obj) = get_units_by_flow_prop(flow_properties, client)
    """
    flow_property_units = {}
    flow_property_obj = {}
    flow_property_units_synonyms = {}

    for flow_property in flow_properties:
        unit_group_id = flow_property.unit_group.id # unit group
        unit_group = client.get(o.UnitGroup, unit_group_id)

        units_in_unit_group = []
        synonyms_in_unit_group = []

        for unit in unit_group.units:
            units_in_unit_group.append(unit.name)
            if unit.synonyms is not None:
                synonyms_in_unit_group.extend(unit.synonyms) # list of lists?

        flow_property_units[flow_property.id] = units_in_unit_group
        flow_property_units_synonyms[flow_property.id] = synonyms_in_unit_group
        flow_property_obj[flow_property.id] = flow_property

    return flow_property_units, flow_property_units_synonyms, flow_property_obj


def get_up_urls():
    """Get a list of NETL DS/DF file URLs.

    Returns
    -------
    list
        A list of URLs.

    Notes
    -----
    Stores the URL list locally in a file called, 'netl_up_urls.txt', to
    avoid unnecessary calls to the website.
    """
    url_file = "netl_up_urls.txt"
    if os.path.isfile(url_file):
        with open(url_file, 'r') as f:
            my_urls = f.readlines()
        # Remove newline character
        my_urls = [x.strip() for x in my_urls]
    else:
        my_urls = scrape_up_library()
        my_data = "\n".join(my_urls)
        with open(url_file, 'w') as f:
            f.write(my_data)

    return my_urls


def import_parameters(p, ds_parameters):
    """Load parameters from DS file into openLCA process object instance.

    Parameters
    ----------
    p : o.Process
        openLCA process object instance
    ds_parameters : pd.Dataframe
        Dataframe containing parameter data
    """
    p.parameters = []

    for param_name in ds_parameters['Parameter Name'].values:
        # Parameter object instantiation
        param_to_add = o.Parameter(
            name = param_name,
            description = ds_parameters[
                ds_parameters['Parameter Name']==param_name
                ]['Comments'].values[0],
            is_input_parameter= None, # Placeholder (see below),
            parameter_scope = o.ParameterScope.PROCESS_SCOPE,
        )

        # If description nan (blank) change to ''
        if type(param_to_add.description) is float:
            param_to_add.description = ''

        # Formula/value processing (Input vs output parameter)
        if pd.isna(
            ds_parameters[ds_parameters['Parameter Name']==param_name]['Formula'].values[0]
        ):
            param_to_add.is_input_parameter = True
            try:
                # if input parameter
                param_to_add.value = float(
                    ds_parameters[
                        ds_parameters['Parameter Name']==param_name
                    ]['Value'].values[0]
                )
            except:
                # do not append broken parameter (likely string value,
                # hopefully doesn't propagate)
                continue
        else:
            param_to_add.is_input_parameter = False
            param_to_add.formula = ds_parameters[ds_parameters['Parameter Name']==param_name]['Formula'].values[0] # if dependent parameter (we shouldn't need value)

        # Completed
        p.parameters.append(param_to_add)


def make_process(work_book):
    # IN PROGRESS
    md = get_metadata(work_book)

    # OPTION 1: convert the metadata workbook into a dictionary and
    # create a process using the 'from_dict' command.
    # OPTION 2: manually extract values from the dataframe.

    p = o.Process()


def metadata_import(p, ds_metadata):
    """Import metadata from DS file into a newly generated openLCA process
    object.

    Parameters
    ----------
    p : o.Process
    ds_metadata : pandas.DataFrame
        Data frame containing Data Summary and Info metadata from DS file.

    Returns
    -------
    tuple
        A tuple of length two.

        - o.Process, a newly created process object.
        - dict, metadata dictionary for future use

    Notes
    -----
    Some fields scaffolded below for future use
    """
    # Could pass this in but it's basically a constant
    client = olca.Client()

    # Converting metadata DF -> dictionary
    md_dict = {}

    for idx, row in ds_metadata.iterrows():
        md_dict[row.Name] = row.Value

    p_cat = "NETL UP Library"
    p_loc = md_dict.get('Geographical Coverage', "United States")
    p_name = md_dict.get("Process Name", "Unnamed Unit Process")
    _description = md_dict['Process Description']+'\n\n'+md_dict['How to Cite This Document'] # Making a description

    p_dict = {
        'name': p_name,
        'description': _description,
        'category': p_cat,
        'processType': o.ProcessType.UNIT_PROCESS,
        'lastChange': _current_time(),
    }

    # Generate the standardized UUID, if absent
    # This makes the UUID reproducible.
    uid = _uid(
        o.ModelType.PROCESS,
        p_cat,
        p_loc,
        p_name)
    p_dict['@id'] = uid

    # Generate new process (using the 'lastChange' from o.Process);
    # can remove later
    p = o.Process().from_dict(p_dict)

    # DQI
    try:
        p.dq_entry = md_dict['DQI']
    except:
        # Some models have no DQ entry
        p.dq_entry = None

    ### GENERAL INFORMATION TAB ###
    p.process_documentation = o.ProcessDocumentation()
    p.version = '01.00.000'

    # Time - Start/end date
    _year = md_dict['Year Data Best Represents']
    # 'YYYY-MM-DD' format - "Year data best represents" field
    p.process_documentation.valid_from = str(_year) + '-01-01'
    # '5 years past "Year data best represents" field
    try:
        p.process_documentation.valid_until = str(int(_year)+5) + '-12-31'
    except:
        pass

    # Scaffolding
    # p.process_documentation.time_description = 'Data best represents ' + str(_year) + '-' + str(int(_year)+5)

    # Geography - US
    # Requires location object w/ correct uuid. Made helper function.
    us_loc_uuid = find_us_location(client)
    p.location = client.get(o.Location, us_loc_uuid)

    #    Technology description (str)
    # p.process_documentation.technology_description =

    ### DOCUMENTATION TAB ###
    # LCI method
    p.process_documentation.inventory_method_description = md_dict.get(
        'Process Scope')

    # Completeness
    p.process_documentation.completeness_description = md_dict['Completeness']

    # Sources - handled by helper functions (e.g., add_references)
    #     Should we also add DF file as a source? DS file?
    #     Only 1 MB for DS/DF files. Do later.

    # Administrative info
    #     Project
    # p.process_documentation.project_description = ''

    #     Intended application
    # p.process_documentation.intended_application = ''

    #     Data set owner - NETL LCA Team
    #     Data generator - NETL LCA Team
    #     Data documentor - NETL LCA Team
    # Need to have the correct instance of the NETL LCA Team 'Actor'

    _netl_actor_uuid = find_netl_actor(client) # Get uuid
    _netl_actor = client.get(o.Actor, _netl_actor_uuid).to_ref()

    p.process_documentation.data_generator = _netl_actor
    p.process_documentation.data_documentor = _netl_actor
    p.process_documentation.data_set_owner = _netl_actor

    #     Publication - create new
    #     (e.g., "2025openLCA conversion of NETL DS/DF files")??

    #     Creation date
    p.process_documentation.creation_date = _current_time()

    #     Access and use restrictions - NETL disclaimer
    p.process_documentation.restrictions_description = 'This project was funded by the Department of Energy, National Energy Technology Laboratory an agency of the United States Government, through a support contract. Neither the United States Government nor any agency thereof, nor any of its employees, nor the support contractor, nor any of their employees, makes any warranty, expressor implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.  Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.'

    # Need to pass this back because it was created here
    return p, md_dict


def metadata_to_dict(md):
    # IN PROGRESS
    #
    # Convert the pandas.DataFrame to a dictionary to be used with openLCA.
    # NOTE: these fall into two categories: Process and ProcessDocumentation

    # Source:
    # https://greendelta.github.io/olca-schema/classes/Process.html
    p_dict = {}
    p_dict['name'] = find_in_metadata(md, 'Process Name')
    p_dict['dqEntry'] = find_in_metadata(md, 'DQI')

    # TODO
    # Needs to be a Ref object for a Location class
    # p_dict['location'] = create_location(
    #     find_in_metadata(md, 'Geographical Coverage'))

    # Source:
    # https://greendelta.github.io/olca-schema/classes/ProcessDocumentation.html
    p_doc = {}


def process_data_scrape(file_path):
    """Pull data from DS file from a given file path.

    Parameters
    ---------
    file_path : str
        Path to DS file .xlsx location

    Returns
    ------
    tuple
        A tuple of length five.

        - pandas.DataFrame, contains DS file Data Summary parameter data
        - pandas.DataFrame, contains DS file Data Summary input flow data
        - pandas.DataFrame, contains DS file Data Summary output flow data
        - pandas.DataFrame, contains DS file Data Summary and Info metadata
        - pandas.DataFrame, contains DS file Reference Source Info data

    Example
    -------
    >>> (ds_parameters, ds_inputs, ds_outputs, ds_metadata,
    ...     ds_references) = process_data_scrape(file_path)

    """
    # Log
    print("...scraping DS file for unit process data...")
    
    ds_file = file_path # Renaming for convenience

    # Get data from DS file
    ds_parameters = get_section(ds_file, SECTION_LABELS[1])
    ds_inputs = get_section(ds_file, SECTION_LABELS[2])
    ds_outputs = get_section(ds_file, SECTION_LABELS[3])
    ds_metadata = get_metadata(ds_file)
    ds_references = get_refs(ds_file)
        

    # Drop nan flows (not necessarily actually causes bugs. Looks nicer though.)
    ds_outputs = ds_outputs.dropna(subset=['Total'])
    ds_inputs = ds_inputs.dropna(subset=['Total'])

    # Make sure there's a unit
    # Added 04/04 when it was noticed that a lot of ref flows were missing units
    if (ds_outputs.iloc[0]['Unit'] is not str) and (ds_outputs.iloc[0]['Units per RF'] is not str):
        unit_str = ds_metadata[ds_metadata['Name']=='Functional unit: Unit']['Value'].values[0]
        ds_outputs.loc[ds_outputs.index[0], 'Units'] = unit_str
        ds_outputs.loc[ds_outputs.index[0], 'Units per RF'] = unit_str


    return ds_parameters, ds_inputs, ds_outputs, ds_metadata, ds_references


def process_import(file_path, overwrite : bool = False):
    """Generate openLCA unit process from DS file.

    Parameters
    ---------
    file_path : str
        Path to DS file .xlsx location.
    overwrite: bool
        If true: Overwrite existing processes w/ same name
        
    Returns
    ------
    p : o.Process
    
    """

    ### Check for existence ###
    if not overwrite:
        found = search_for_process(file_path) # True if found
        if found:
            print("...Skipping...")
            return #exit

    print("...Importing...")
    
    ### PROCESS CREATION/DATA PULLING ###
    # Start openLCA IPC server
    # WARNING: no errors if the IPC service isn't running until
    # `find_us_location` is called.
    logging.info("Connecting to IPC server on port %d" % IPC_PORT)
    client = olca.Client(IPC_PORT)

    # Initialize o.Process object
    logging.info("Initializing new unit process.")
    p = o.Process()

    # Scrape data from DS file .xlsx document
    all_data = process_data_scrape(file_path)
    ds_parameters = all_data[0]
    ds_inputs = all_data[1]
    ds_outputs = all_data[2]
    ds_metadata = all_data[3]
    ds_references = all_data[4]

    ### FORMAT PARAMETERS ###
    # Avoids errors with parameters starting with numbers or having string values (both incompatible with openLCA)
    ds_inputs, ds_outputs, ds_parameters = format_parameters(ds_inputs, ds_outputs, ds_parameters)
    
    ### METADATA ###
    # Import metadata using helper function
    p, md_dict = metadata_import(p, ds_metadata)

    ### DQA ###
    # Add DQA methods to process (flow/process) and sets matrix values for
    # process
    add_process_dqa(md_dict, p, client)

    ### PARAMETERS ###
    import_parameters(p, ds_parameters)

    ### EXCHANGES ###
    # aka "flows"
    # Adding input flows and associated data to process p
    data = get_flow_data(ds_inputs, True, client)
    p = add_flow_data(data, p, client, True)

    # Adding output flows and associated data to process p
    data = get_flow_data(ds_outputs, False, client)
    p = add_flow_data(data, p, client, False)

    # Add references
    try:
        p1 = add_references(file_path, p, client)

        if p1 is not None:
            p = p1
        
    except Exception as e:
        # No references perhaps?
        print(f"Error adding references in process_import:\n{e}")



    ### FINAL ###
    # Delete (if any) existing versions of p in the client
    client.delete(p)

    # Add new version of p to client
    try:
        client.put(p)
        # Log
        print(f"...process {p.name} added to database...")
    except Exception as e:
        print(e)
    
    return p


def scrape_up_library():
    """Scrape the NETL Unit Process Library using Selenium

    Returns
    -------
    list
        A list of all urls attached to DS/DF buttons on the page.

    Notes
    -----
    May pop open a browser window temporarily as the data from the website
    is scraped; don't close the window!
    """
    # Set up WebDriver
    url = "https://netl.doe.gov/node/2573" # URL of NETL UP Library
    driver = webdriver.Chrome()  # Ensure you have ChromeDriver installed
    driver.get(url)

    # Wait for iframe to load
    # (might need to adjust depending on internet speed/server response time)
    time.sleep(10)

    # Scrape data
    iframe = driver.find_element(By.TAG_NAME, "iframe")
    driver.switch_to.frame(iframe)

    # Find all buttons inside the iframe
    buttons = driver.find_elements(By.CLASS_NAME, "btn.btn-primary")

    urls = [
        button.get_attribute("href") for button in buttons
        if button.get_attribute("href")
    ]

    driver.quit()

    return urls


##############################################################################
# MAIN
##############################################################################
if __name__ == '__main__':
    import os

    my_ds_file = "DS_O_Cement_with_Optional_Capture_2023.03.xlsx"
    if os.path.isfile(my_ds_file):
        md = get_metadata(my_ds_file, False)
